{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction Kaggle\n",
    "- Minh Nguyen\n",
    "- 11/20/2024\n",
    "- https://www.kaggle.com/c/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1168, 330), Validation data shape: (292, 330)\n",
      "Test data shape: (1459, 330)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load dataset\n",
    "train_data = pd.read_csv(\"data/kaggle_house/train.csv\")\n",
    "test_data = pd.read_csv(\"data/kaggle_house/test.csv\")\n",
    "\n",
    "# Drop ID and separate label\n",
    "train_data.drop(columns=['Id'], inplace=True)\n",
    "test_ids = test_data['Id']\n",
    "test_data.drop(columns=['Id'], inplace=True)\n",
    "\n",
    "# Identify numeric columns excluding 'SalePrice'\n",
    "numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols = numeric_cols.drop('SalePrice')  # Drop 'SalePrice' as it doesn't exist in test_data\n",
    "\n",
    "# Fill missing values with column mean for both datasets\n",
    "train_data[numeric_cols] = train_data[numeric_cols].fillna(train_data[numeric_cols].mean())\n",
    "test_data[numeric_cols] = test_data[numeric_cols].fillna(test_data[numeric_cols].mean())\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "train_data = pd.get_dummies(train_data, dummy_na=True)\n",
    "test_data = pd.get_dummies(test_data, dummy_na=True)\n",
    "\n",
    "# Save SalePrice separately\n",
    "labels = np.log1p(train_data['SalePrice'].values)  # Use log-transformed prices\n",
    "train_data.drop(columns=['SalePrice'], inplace=True)\n",
    "\n",
    "# Align train and test data (Exclude SalePrice from the alignment operation)\n",
    "train_data, test_data = train_data.align(test_data, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(train_data.values)\n",
    "test_data = scaler.transform(test_data.values)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Debug output to verify dimensions\n",
    "print(f\"Training data shape: {X_train.shape}, Validation data shape: {X_val.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 330])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "class HouseDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32) if labels is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.features[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HouseDataset(X_train, y_train)\n",
    "val_dataset = HouseDataset(X_val, y_val)\n",
    "test_dataset = HouseDataset(test_data)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for X, y in train_loader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size, dropout_rate=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer_1 = nn.Linear(input_size, hidden_size_1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.hidden_layer_2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.output_layer = nn.Linear(hidden_size_2, output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.hidden_layer_1(X) # 330 x hidden_size 1\n",
    "        X = self.activation(X)\n",
    "        X = self.dropout(X)\n",
    "        X = self.hidden_layer_2(X) # hidden_size 1 x hidden_size 2\n",
    "        X = self.activation(X)\n",
    "        X = self.dropout(X)\n",
    "        logits = self.output_layer(X)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnguyen0226/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mnguyen0226/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mnguyen0226/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([36])) that is different to the input size (torch.Size([36, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 127.2579, Val Loss: 96.4406\n",
      "Epoch 2/50, Train Loss: 54.5261, Val Loss: 12.1233\n",
      "Epoch 3/50, Train Loss: 8.7659, Val Loss: 3.5954\n",
      "Epoch 4/50, Train Loss: 4.9562, Val Loss: 3.0329\n",
      "Epoch 5/50, Train Loss: 3.7064, Val Loss: 2.3235\n",
      "Epoch 6/50, Train Loss: 3.1308, Val Loss: 2.4232\n",
      "Epoch 7/50, Train Loss: 3.0921, Val Loss: 2.4270\n",
      "Epoch 8/50, Train Loss: 2.6676, Val Loss: 2.1118\n",
      "Epoch 9/50, Train Loss: 2.8439, Val Loss: 2.1467\n",
      "Epoch 10/50, Train Loss: 2.8111, Val Loss: 2.1340\n",
      "Epoch 11/50, Train Loss: 2.6964, Val Loss: 1.9729\n",
      "Epoch 12/50, Train Loss: 2.6093, Val Loss: 2.4092\n",
      "Epoch 13/50, Train Loss: 2.5766, Val Loss: 1.8561\n",
      "Epoch 14/50, Train Loss: 2.5639, Val Loss: 2.0240\n",
      "Epoch 15/50, Train Loss: 2.5398, Val Loss: 2.1079\n",
      "Epoch 16/50, Train Loss: 2.6829, Val Loss: 2.0138\n",
      "Epoch 17/50, Train Loss: 2.6460, Val Loss: 2.0189\n",
      "Epoch 18/50, Train Loss: 2.4348, Val Loss: 2.2694\n",
      "Epoch 19/50, Train Loss: 2.4555, Val Loss: 2.2041\n",
      "Epoch 20/50, Train Loss: 2.3184, Val Loss: 2.0577\n",
      "Epoch 21/50, Train Loss: 2.3206, Val Loss: 1.9214\n",
      "Epoch 22/50, Train Loss: 2.2924, Val Loss: 1.9725\n",
      "Epoch 23/50, Train Loss: 2.3274, Val Loss: 2.1867\n",
      "Epoch 24/50, Train Loss: 2.3409, Val Loss: 1.8842\n",
      "Epoch 25/50, Train Loss: 2.2871, Val Loss: 2.0870\n",
      "Epoch 26/50, Train Loss: 2.3932, Val Loss: 2.0871\n",
      "Epoch 27/50, Train Loss: 2.3141, Val Loss: 2.1878\n",
      "Epoch 28/50, Train Loss: 2.5413, Val Loss: 2.0117\n",
      "Epoch 29/50, Train Loss: 2.1982, Val Loss: 2.1526\n",
      "Epoch 30/50, Train Loss: 2.2623, Val Loss: 1.9991\n",
      "Epoch 31/50, Train Loss: 2.3670, Val Loss: 2.1871\n",
      "Epoch 32/50, Train Loss: 2.4216, Val Loss: 2.0202\n",
      "Epoch 33/50, Train Loss: 2.2830, Val Loss: 1.9250\n",
      "Epoch 34/50, Train Loss: 2.1913, Val Loss: 1.8277\n",
      "Epoch 35/50, Train Loss: 2.1809, Val Loss: 2.0671\n",
      "Epoch 36/50, Train Loss: 2.1493, Val Loss: 2.0942\n",
      "Epoch 37/50, Train Loss: 2.2239, Val Loss: 2.1321\n",
      "Epoch 38/50, Train Loss: 2.2727, Val Loss: 1.9917\n",
      "Epoch 39/50, Train Loss: 2.1476, Val Loss: 1.9229\n",
      "Epoch 40/50, Train Loss: 2.0024, Val Loss: 2.1894\n",
      "Epoch 41/50, Train Loss: 2.0257, Val Loss: 2.2383\n",
      "Epoch 42/50, Train Loss: 2.2592, Val Loss: 2.0184\n",
      "Epoch 43/50, Train Loss: 2.1104, Val Loss: 2.0416\n",
      "Epoch 44/50, Train Loss: 2.2249, Val Loss: 2.0866\n",
      "Epoch 45/50, Train Loss: 2.1692, Val Loss: 1.9060\n",
      "Epoch 46/50, Train Loss: 2.2146, Val Loss: 1.8628\n",
      "Epoch 47/50, Train Loss: 2.1741, Val Loss: 1.8196\n",
      "Epoch 48/50, Train Loss: 2.0344, Val Loss: 2.2328\n",
      "Epoch 49/50, Train Loss: 2.2252, Val Loss: 2.2943\n",
      "Epoch 50/50, Train Loss: 2.2208, Val Loss: 1.7993\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Initialize and train the model\n",
    "input_dim = X_train.shape[1] # 330\n",
    "model = MLP(input_size=input_dim, hidden_size_1=128, hidden_size_2=64, output_size=1)\n",
    "train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            y_pred = model(X_batch).numpy().flatten()  # Ensure predictions are 1D\n",
    "            predictions.extend(y_pred)\n",
    "    return predictions\n",
    "\n",
    "# Predict and generate submission file\n",
    "predictions = predict(model, test_loader)\n",
    "predictions = np.expm1(predictions)  # Reverse log transformation\n",
    "\n",
    "# Ensure predictions are 1D\n",
    "predictions = np.array(predictions).ravel()  # Flatten if needed\n",
    "submission = pd.DataFrame({'Id': test_ids, 'SalePrice': predictions})\n",
    "\n",
    "submission.to_csv('data/kaggle_house/submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
