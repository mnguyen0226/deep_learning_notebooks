{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Branch Networks (GoogLeNet)\n",
    "- Minh Nguyen\n",
    "- 1/25/2025\n",
    "- Idea: The core innovation of GoogLeNet is the Inception module, which allows the network to efficiently capture features at multiple scales. Each module consists of parallel convolutional layers with different filter sizes (1x1, 3x3, 5x5) and a max pooling operation. Their outputs are concatenated, providing a ricker feature representation.\n",
    "- Key Features:\n",
    "    - Using repeated NiN, reapeted block and cocktail of convolution kernels.\n",
    "    - Design pattern: the stem is given by the first 2 or 3 convolutions that operate on the image, they extract low-level features from the underlying images. This is followed by a body of convolutional blocks. Finally, the head maps the features obtained so far to the required classification, segmentation, detection, or tracking problem at handd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             160\n",
      "            Conv2d-2           [-1, 16, 28, 28]             272\n",
      "            Conv2d-3           [-1, 16, 28, 28]           2,320\n",
      "            Conv2d-4           [-1, 16, 28, 28]           6,416\n",
      "         MaxPool2d-5           [-1, 16, 28, 28]               0\n",
      "            Conv2d-6           [-1, 16, 28, 28]             272\n",
      "   SimpleInception-7           [-1, 64, 28, 28]               0\n",
      "            Conv2d-8           [-1, 16, 14, 14]           1,040\n",
      "            Conv2d-9           [-1, 16, 14, 14]           9,232\n",
      "           Conv2d-10           [-1, 16, 14, 14]          25,616\n",
      "        MaxPool2d-11           [-1, 64, 14, 14]               0\n",
      "           Conv2d-12           [-1, 16, 14, 14]           1,040\n",
      "  SimpleInception-13           [-1, 64, 14, 14]               0\n",
      "           Linear-14                   [-1, 10]          31,370\n",
      "================================================================\n",
      "Total params: 77,738\n",
      "Trainable params: 77,738\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.24\n",
      "Params size (MB): 0.30\n",
      "Estimated Total Size (MB): 1.54\n",
      "----------------------------------------------------------------\n",
      "Epoch 1/5, Loss: 0.4001\n",
      "Epoch 2/5, Loss: 0.2610\n",
      "Epoch 3/5, Loss: 0.2193\n",
      "Epoch 4/5, Loss: 0.1905\n",
      "Epoch 5/5, Loss: 0.1668\n",
      "Test Accuracy: 91.03%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "# -----------------------------\n",
    "# Inception Module (Simplified)\n",
    "# -----------------------------\n",
    "class SimpleInception(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SimpleInception, self).__init__()\n",
    "        # 1x1 convolution branch\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        # 3x3 convolution branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # 5x5 convolution branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        # Max pooling branch followed by 1x1 convolution\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch_pool = self.branch_pool(x)\n",
    "        \n",
    "        # Concatenate all branches along the channel dimension\n",
    "        outputs = torch.cat([branch1x1, branch3x3, branch5x5, branch_pool], dim=1)\n",
    "        return outputs\n",
    "\n",
    "# -----------------------------\n",
    "# Simple Network with Inception\n",
    "# -----------------------------\n",
    "class SimpleInceptionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleInceptionNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # Initial Convolution\n",
    "        self.inception1 = SimpleInception(16)  # First Inception block\n",
    "        self.inception2 = SimpleInception(64)  # Second Inception block\n",
    "        self.fc = nn.Linear(64 * 7 * 7, 10)  # Fully connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.inception1(x)\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)  # Downsample\n",
    "        x = self.inception2(x)\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)  # Downsample\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Data Loading and Preparation\n",
    "# -----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# View Model Architecture\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleInceptionNet().to(device)\n",
    "\n",
    "# Use torchsummary to view architecture\n",
    "summary(model, input_size=(1, 28, 28))\n",
    "\n",
    "# -----------------------------\n",
    "# Training Setup\n",
    "# -----------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Loop\n",
    "# -----------------------------\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Testing Loop\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/8_4_inception_net.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Example input for the model\n",
    "x = torch.randn(1, 1, 28, 28).to(device)\n",
    "\n",
    "# Get the graph of the model\n",
    "model_graph = make_dot(model(x), params=dict(model.named_parameters()))\n",
    "\n",
    "# Save the graph or render it\n",
    "model_graph.render(\"images/8_4_inception_net\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
